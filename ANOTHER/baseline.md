# Построение базовой модели

## О самой задаче

Итак, мы имеем ЭКГ-сигнал и базовые данные пациента (возраст и пол), на основе которых нам необходимо определить наличие патологии на ЭКГ и ее вид ИЛИ вид нормы. **Таким образом**, мы решаем задачу `многоклассовой классификации`. Вид задачи и будет определять семейство моделей для решения нашей задачи. 

## Наши шаги

1. Селекция семейств моделей, подходящих для решения задачи. 
2. Краткая теоретическая выкладка по каждой. 
3. Обучение моделей, выбор метрик для сравнения. 
4. Отбор лучшей модели, обоснование выбора.

## 1-2 Выбранные модели и их pros и cons

#### Логистическая регрессия

##### Основные характеристики

- Логистическая регрессия является линейной моделью, которая применяется для прогнозирования вероятностей принадлежности объектов к классам.
- Применима как для бинарной, так и для многоклассовой классификации.
- Использует логистическую функцию для преобразования выводов линейной комбинации признаков в вероятности.
- Поддерживает различные типы регуляризации (L1, L2, elasticnet) для предотвращения переобучения модели.
- Для многоклассовой классификации могут использоваться различные подходы, такие как 'one-vs-rest' (OvR) или 'multinomial'.

##### Плюсы

1. **Интерпретируемость**: Логистическая регрессия обеспечивает хорошую интерпретируемость, так как коэффициенты могут быть легко поняты и проанализированы.
2. **Простота в реализации**: Реализация логистической регрессии является относительно простой и быстрой по сравнению с более сложными моделями.
3. **Регуляризация**: Возможность применения регуляризации для уменьшения переобучения, что является особенно важным при наличии большого количества признаков.
4. **Эффективность на небольших наборах данных**: Логистическая регрессия может показывать хорошие результаты на малых объемах данных.
5. **Подходит для линейных зависимостей**: Логистическая регрессия эффективна, когда классы линейно разделимы.

##### Минусы

1. **Линейность**: Ограниченность в способности модели захватывать сложные, не линейные зависимости между признаками.
2. **Чувствительность к мультиколлинеарности**: Присутствие высококоррелированных признаков может негативно повлиять на качество модели.
3. **Требование к обрабатываемым данным**: В случае дисбаланса классов модель может показывать низкую точность, если не учитывать классы в прогнозировании (можно решать с помощью настройки параметра `class_weight`).
4. **Ограниченные возможности для многоклассовой классификации**: она может быть менее эффективной в сравнении с методами, специально разработанными для этих задач (например, SVM с RBF ядром).


#### Метод опорных векторов (SVM)

##### Основные характеристики

- SVM — это алгоритм машинного обучения, используемый для классификации и регрессии, основанный на поиске гиперплоскости, максимизирующей разделение классов.
- Может использовать различные ядра (линейное, полиномиальное, RBF и др.) для обработки линейных и нелинейных данных.
- В случае многоклассовой классификации можно использовать подходы 'one-vs-one' (OvO) и 'one-vs-rest' (OvR).
- Обратная связь между относительными позициями классов и прагматичными границами делает методы SVM особенно подходящими для высокоразмерных данных.

##### Плюсы

1. **Эффективность в высоких измерениях**: SVM хорошо работает с высокоразмерными данными, что делает его отличным выбором для задач с большим количеством признаков.
2. **Гибкость через ядра**: Возможность использования различных ядер позволяет SVM обнаруживать сложные нелинейные зависимости между признаками

3. ## 1. Метрики для оценки моделей

#### Общие метрики для логистической регрессии и SVM

1. **Accuracy (Точность)**
Определяет долю правильно предсказанных классов к общему числу наблюдений.
     ```
     Accuracy = (TP + TN) / (TP + TN + FP + FN)
     ```
<span style="color:blue;">***Примечание:***</span> Показатель может быть вводящим в заблуждение при наличии дисбаланса классов.

2. **Precision (Точность)**
Отражает долю истинных положительных наблюдений к всем предсказанным положительным. 
     ```
     Precision = TP / (TP + FP)
     ```

3. **Recall (Полнота)**
Показатель того, сколько из всех позитивных классов было правильно предсказано.
     ```
     Recall = TP / (TP + FN)
     ```

4. **F1-Score**
Гармоническое среднее между точностью и полнотой.
     ```
     F1 = 2 * (Precision * Recall) / (Precision + Recall)
     ```
<span style="color:green;">***Примечание:***</span> Эффективная метрика, когда важно учитывать баланс между точностью и полнотой.

5. **Log Loss (Логарифмическая функция потерь)**
Оценивает, насколько хорошо вероятностные предсказания модели соответствуют истинным значениям классов.
<span style="color:red;">***Чем ниже, тем лучше!***</span>

#### Уникальные метрики

- **Для логистической регрессии:**
**AUC-ROC (Площадь под кривой)**
Отражает способность модели различать классы на всех уровнях разделения. Чем выше AUC, тем лучше модель.
<span style="color:purple;">***Идеально: AUC = 1***</span>
  
- **Для SVM:**
**Скорость изменения зазора (Margin)**
Оценивает зазор между классами, который SVM стремится максимизировать.
<span style="color:orange;">***Более широкий зазор — лучше***</span>

#### Решение по выбору метрик для сравнения моделей

Для сравнительной оценки логистической регрессии и SVM принято решение использовать **F1-точность** или **Log Loss**:

- <span style="color:darkgreen;">***F1-точность:***</span> Хорошо подходит для сбалансированных задач, где важно учитывать как точность, так и полноту.
- <span style="color:darkred;">***Log Loss:***</span> Особенно полезен при анализе вероятностных предсказаний, которые критичны для логистической регрессии.

Если данные несбалансированы, как в нашем случае, F1-точность может быть предпочтительнее. Но это не отменяет использование других метрик для сравнения моделей между собой внутри своих семейств.

3. ## 2. Обучение моделей


#### 1. Препроцессинг данных

Перед обучением моделей мы подготовили данные следующим образом:
- **Фильтрация данных:** Использовали данные, относящиеся к 15 заболеваниям.
- **Стандартизация признаков:** Применили стандартизацию с использованием `StandardScaler` для нормализации данных.

```python
sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)
```

#### 2. Обученные модели

Мы обучили несколько моделей, включая:

##### 2.1 Логистическая регрессия (Logistic Regression)

- **Гиперпараметры:**
  - `penalty`: {'l1', 'l2', 'elasticnet', 'none'}
  - `tol`: 1e-4 (по умолчанию)
  - `C`: 1.0 (по умолчанию)
  - `fit_intercept`: True (по умолчанию)
  - `class_weight`: balanced
  - `solver`: saga
  - `multi_class`: ovr

```python
lg = LogisticRegression(multi_class='ovr', solver='saga', class_weight='balanced', random_state=RAND)
lg.fit(X_train_std, y_train)
```

- **Результаты:**
  - **Accuracy:** 0.420746 (train), 0.359368 (test)
  - **Precision:** 0.457677 (train), 0.399890 (test)
  - **Recall:** 0.420746 (train), 0.359368 (test)
  - **F1:** 0.426536 (train), 0.368385 (test)
  - **Logloss:** 2.053606 (train), 2.213359 (test)

---

##### 2.2 Логистическая регрессия с L1 регуляризацией

- **Гиперпараметры:**
  - `penalty='elasticnet'`
  - `l1_ratio=0.5`

```python
lg_l1 = LogisticRegression(multi_class='ovr', random_state=RAND, penalty='elasticnet', l1_ratio=0.5, solver='saga', class_weight='balanced')
lg_l1.fit(X_train_std, y_train)
```

- **Результаты:**
  - **Accuracy:** 0.420603 (train), 0.360222 (test)
  - **Результаты в целом аналогичны логистической регрессии без регуляризации.**

---

##### 2.3 Метод опорных векторов (SVM)

- **Гиперпараметры:**
  - `kernel`: linear, затем rbf (после GridSearchCV)
  - `decision_function_shape`: ovr

```python
svm = SVC(probability=True, decision_function_shape='ovr', kernel='linear', random_state=RAND)
svm.fit(X_train_std, y_train)
```
#### Результаты SVM

- **Accuracy:** 0.720831 (train), 0.395647 (test)
- **Precision:** 0.720668 (train), 0.372630 (test)
- **Logloss:** 1.672277 (train), 1.949848 (test)

---

#### 3. Подходы к многоклассовой классификации

Для решения задачи многоклассовой классификации мы выбрали подход **"один против остальных" (OVR)**. Этот подход позволяет создать отдельную бинарную классификацию для каждого класса, что увеличивает гибкость и точность моделей.

---

#### 4. Регуляризация и ядра

- **Логистическая регрессия:** использовали `saga`, совместимый со всеми типами регуляризации:
  - `'l1', 'l2', 'elasticnet', 'none'`

- **SVM:** Применяли ядра `linear` и `rbf`. Модель с ядром `rbf` показала лучшие результаты на отложенной выборке.

#### 5. Саммари по результатам обучения моделей, сводное сравнение

| Номер | Модель                     | Accuracy               | Precision               | Recall                 | F1                     | Logloss                |
|-------|----------------------------|------------------------|-------------------------|------------------------|------------------------|------------------------|
| 0     | LogisticRegression_train    | <span style="color:green;">0.420746</span>   | <span style="color:green;">0.457677</span>   | <span style="color:green;">0.420746</span>   | <span style="color:green;">0.426536</span>   | <span style="color:red;">2.053606</span>   |
| 1     | LogisticRegression_test     | <span style="color:green;">0.359368</span>   | <span style="color:green;">0.399890</span>   | <span style="color:green;">0.359368</span>   | <span style="color:green;">0.368385</span>   | <span style="color:red;">2.213359</span>   |
| 2     | LogisticRegression_L1_train | <span style="color:green;">0.420603</span>   | <span style="color:green;">0.457294</span>   | <span style="color:green;">0.420603</span>   | <span style="color:green;">0.426216</span>   | <span style="color:red;">2.054725</span>   |
| 3     | LogisticRegression_L1_test  | <span style="color:green;">0.360222</span>   | <span style="color:green;">0.401708</span>   | <span style="color:green;">0.360222</span>   | <span style="color:green;">0.369712</span>   | <span style="color:red;">2.213272</span>   |
| 4     | SVM_train                   | <span style="color:red;">0.720831</span>    | <span style="color:red;">0.720668</span>    | <span style="color:red;">0.720831</span>    | <span style="color:red;">0.710104</span>    | <span style="color:red;">1.672277</span>    |
| 5     | SVM_test                    | <span style="color:green;">0.395647</span>   | <span style="color:green;">0.372630</span>   | <span style="color:green;">0.395647</span>   | <span style="color:green;">0.379695</span>   | <span style="color:red;">1.949848</span>   |
| 6     | SVM_best_train              | <span style="color:red;">0.666477</span>     | <span style="color:red;">0.741507</span>     | <span style="color:red;">0.666477</span>     | <span style="color:red;">0.611807</span>     | <span style="color:red;">1.059538</span>     |
| 7     | SVM_best_test               | <span style="color:green;">0.435766</span>   | <span style="color:green;">0.337341</span>   | <span style="color:green;">0.435766</span>   | <span style="color:green;">0.367242</span>   | <span style="color:red;">1.796744</span>   |

## 4. Выбор лучшей модели и обоснование выбора

Наилучшие результаты демонстрирует модель SVM с ядром rbf:

- Улучшенные метрики по сравнению с логистической регрессией, особенно в Logloss.
- Низкие значения Logloss и высокая Accuracy указывают на способность модели к хорошему обобщению.
